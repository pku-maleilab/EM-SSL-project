{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Napari-OmniEM Interactive electron microscopy analysis powered by foundation models. Napari-OmniEM is a Napari plugin that provides a unified, GPU-accelerated interface for electron microscopy (EM) image processing. Built on top of EM-DINO and OmniEM , it enables scalable, high-quality inference across heterogeneous 2D and 3D EM datasets, bridging foundation models with practical, interactive EM workflows. Napari-OmniEM supports both in-memory and on-disk (multi-GPU) inference, and task-aware model management , making it suitable for exploratory analysis, large-volume processing, and downstream quantitative studies. Installation Getting Started Model Zoo Related Projects EM-SSL Project : https://github.com/pku-maleilab/EM-SSL-project","title":"Home"},{"location":"#napari-omniem","text":"Interactive electron microscopy analysis powered by foundation models. Napari-OmniEM is a Napari plugin that provides a unified, GPU-accelerated interface for electron microscopy (EM) image processing. Built on top of EM-DINO and OmniEM , it enables scalable, high-quality inference across heterogeneous 2D and 3D EM datasets, bridging foundation models with practical, interactive EM workflows. Napari-OmniEM supports both in-memory and on-disk (multi-GPU) inference, and task-aware model management , making it suitable for exploratory analysis, large-volume processing, and downstream quantitative studies. Installation Getting Started Model Zoo","title":"Napari-OmniEM"},{"location":"#_1","text":"","title":""},{"location":"#related-projects","text":"EM-SSL Project : https://github.com/pku-maleilab/EM-SSL-project","title":"Related Projects"},{"location":"license/","text":"","title":"License"},{"location":"more/","text":"","title":"More links"},{"location":"release-notes/","text":"","title":"Release Notes"},{"location":"model-zoo/","text":"","title":"Overview"},{"location":"user-guide/","text":"Getting Started Installation In-memory Inference Local Inference Sliding Window Parameters Task and Solution Manage","title":"Index"},{"location":"user-guide/getting-started/","text":"Getting Started with Napari-OmniEM This guide provides a quick introduction to Napari-OmniEM and helps you get up and running with your first inference task. Installation Currently, Napari-OmniEM is supported only via source installation . Please follow the instructions in the Installation Guide . Launch Napari and OmniEM Launch napari from the command line napari Enable the OmniEM plugin In the napari menu bar, open Plugins and select OmniEM . Load sample EM data Napari-OmniEM provides built-in sample electron microscopy datasets for quick testing. Load sample files directly from the OmniEM panel: Alternatively, load your own data via File \u2192 Open File(s) in the napari menu. Interface overview After loading the plugin and EM data, click the \ud83d\udd04 Refresh/Register button in the Napari-OmniEM panel to register the currently loaded data. Once registered and selected a data, the napari interface should appear similar to the following: Next Steps Continue with the detailed user guides to explore OmniEM features: In-memory Inference Local Inference Sliding Window Parameters Task and Solution Manage","title":"Getting Started"},{"location":"user-guide/getting-started/#getting-started-with-napari-omniem","text":"This guide provides a quick introduction to Napari-OmniEM and helps you get up and running with your first inference task.","title":"Getting Started with Napari-OmniEM"},{"location":"user-guide/getting-started/#installation","text":"Currently, Napari-OmniEM is supported only via source installation . Please follow the instructions in the Installation Guide .","title":"Installation"},{"location":"user-guide/getting-started/#launch-napari-and-omniem","text":"","title":"Launch Napari and OmniEM"},{"location":"user-guide/getting-started/#launch-napari-from-the-command-line","text":"napari","title":"Launch napari from the command line"},{"location":"user-guide/getting-started/#enable-the-omniem-plugin","text":"In the napari menu bar, open Plugins and select OmniEM .","title":"Enable the OmniEM plugin"},{"location":"user-guide/getting-started/#load-sample-em-data","text":"Napari-OmniEM provides built-in sample electron microscopy datasets for quick testing. Load sample files directly from the OmniEM panel: Alternatively, load your own data via File \u2192 Open File(s) in the napari menu.","title":"Load sample EM data"},{"location":"user-guide/getting-started/#interface-overview","text":"After loading the plugin and EM data, click the \ud83d\udd04 Refresh/Register button in the Napari-OmniEM panel to register the currently loaded data. Once registered and selected a data, the napari interface should appear similar to the following:","title":"Interface overview"},{"location":"user-guide/getting-started/#next-steps","text":"Continue with the detailed user guides to explore OmniEM features: In-memory Inference Local Inference Sliding Window Parameters Task and Solution Manage","title":"Next Steps"},{"location":"user-guide/in-memory/","text":"In-memory inference In-memory inference is performed directly within the main Napari-OmniEM panel and operates on image data loaded as napari image layers. All computation is executed in memory, making this mode ideal for interactive exploration, debugging, and small-to-medium EM datasets that fit into GPU/CPU memory. Quick Start (Minimal Steps) Open napari and load an EM image or volume. Open Plugins \u2192 OmniEM . Click \ud83d\udd04 Refresh / Register to detect loaded data. Select Data , Task , and Solution . Click Run \u2192 confirm \u2192 view results in napari. For more control and advanced settings, see the detailed steps below. Step 1: Select data Load image data into napari via File \u2192 Open File(s) . The data will appear as one or more image layers . Click the \ud83d\udd04 Refresh / Register button in the Napari-OmniEM panel to register the currently loaded image layers. Select the target image from the Data dropdown list. If the selected data is a 3D volume , specify the z-dimension : By default, the axis with the minimum side length is automatically chosen. You can override this if your data uses a different axis convention. Step 2: Select task and solution Choose a Task corresponding to the intended processing goal (e.g., segmentation, restoration). Select a compatible Solution from the Solution list. Only solutions that are compatible with the selected data (dimensionality, modality, and task) will be shown. For detailed descriptions of available tasks and solutions, refer to the Model Zoo . Terminology note - Task : defines what problem is being solved. - Solution : a concrete model + configuration that solves a task. This terminology is consistent across in-memory and local inference modes. Step 3 advanced settings for sliding window inference In-memory inference uses the same sliding window mechanism as local inference. Adjust sliding window hyperparameters (e.g., input size, overlap, batch size) as needed. For detailed explanations and recommended values, see: Sliding Window Parameters If you are unsure, the default parameters are generally safe for first-time use. Step 4: Run inference Click the Run button to open the inference dialog. Review: Estimated number of batches Region-of-interest (ROI) size Click Run in the dialog to start inference. Once inference finishes, close the dialog to view results directly in napari as new layers. Tips & Common Pitfalls Memory usage In-memory inference loads data and intermediate results into memory. For large 3D volumes, GPU memory can be exhausted quickly. If you encounter out-of-memory errors: Reduce batch size (even 1 is acceptable) Reduce input window size Consider using Local Inference instead Batch size Batch size primarily affects memory usage, not output quality. Small batch sizes are recommended for large EM data. Refresh/Register button Always click \ud83d\udd04 Refresh / Register after: Loading new data Removing image layers Renaming layers Failure to do so may result in missing or outdated data selections. When to use Local Inference instead Use Local Inference if: The dataset is too large to fit into memory You want multi-GPU parallel inference You want to save results directly to disk (e.g., TIFF, Zarr, Dask) Related Pages Local Inference Sliding Window Parameters Task and Solution Manage Model Zoo","title":"In-memory Inference"},{"location":"user-guide/in-memory/#in-memory-inference","text":"In-memory inference is performed directly within the main Napari-OmniEM panel and operates on image data loaded as napari image layers. All computation is executed in memory, making this mode ideal for interactive exploration, debugging, and small-to-medium EM datasets that fit into GPU/CPU memory.","title":"In-memory inference"},{"location":"user-guide/in-memory/#quick-start-minimal-steps","text":"Open napari and load an EM image or volume. Open Plugins \u2192 OmniEM . Click \ud83d\udd04 Refresh / Register to detect loaded data. Select Data , Task , and Solution . Click Run \u2192 confirm \u2192 view results in napari. For more control and advanced settings, see the detailed steps below.","title":"Quick Start (Minimal Steps)"},{"location":"user-guide/in-memory/#step-1-select-data","text":"Load image data into napari via File \u2192 Open File(s) . The data will appear as one or more image layers . Click the \ud83d\udd04 Refresh / Register button in the Napari-OmniEM panel to register the currently loaded image layers. Select the target image from the Data dropdown list. If the selected data is a 3D volume , specify the z-dimension : By default, the axis with the minimum side length is automatically chosen. You can override this if your data uses a different axis convention.","title":"Step 1: Select data"},{"location":"user-guide/in-memory/#step-2-select-task-and-solution","text":"Choose a Task corresponding to the intended processing goal (e.g., segmentation, restoration). Select a compatible Solution from the Solution list. Only solutions that are compatible with the selected data (dimensionality, modality, and task) will be shown. For detailed descriptions of available tasks and solutions, refer to the Model Zoo . Terminology note - Task : defines what problem is being solved. - Solution : a concrete model + configuration that solves a task. This terminology is consistent across in-memory and local inference modes.","title":"Step 2: Select task and solution"},{"location":"user-guide/in-memory/#step-3-advanced-settings-for-sliding-window-inference","text":"In-memory inference uses the same sliding window mechanism as local inference. Adjust sliding window hyperparameters (e.g., input size, overlap, batch size) as needed. For detailed explanations and recommended values, see: Sliding Window Parameters If you are unsure, the default parameters are generally safe for first-time use.","title":"Step 3 advanced settings for sliding window inference"},{"location":"user-guide/in-memory/#step-4-run-inference","text":"Click the Run button to open the inference dialog. Review: Estimated number of batches Region-of-interest (ROI) size Click Run in the dialog to start inference. Once inference finishes, close the dialog to view results directly in napari as new layers.","title":"Step 4: Run inference"},{"location":"user-guide/in-memory/#tips-common-pitfalls","text":"","title":"Tips &amp; Common Pitfalls"},{"location":"user-guide/in-memory/#memory-usage","text":"In-memory inference loads data and intermediate results into memory. For large 3D volumes, GPU memory can be exhausted quickly. If you encounter out-of-memory errors: Reduce batch size (even 1 is acceptable) Reduce input window size Consider using Local Inference instead","title":"Memory usage"},{"location":"user-guide/in-memory/#batch-size","text":"Batch size primarily affects memory usage, not output quality. Small batch sizes are recommended for large EM data.","title":"Batch size"},{"location":"user-guide/in-memory/#refreshregister-button","text":"Always click \ud83d\udd04 Refresh / Register after: Loading new data Removing image layers Renaming layers Failure to do so may result in missing or outdated data selections.","title":"Refresh/Register button"},{"location":"user-guide/in-memory/#when-to-use-local-inference-instead","text":"Use Local Inference if: The dataset is too large to fit into memory You want multi-GPU parallel inference You want to save results directly to disk (e.g., TIFF, Zarr, Dask)","title":"When to use Local Inference instead"},{"location":"user-guide/in-memory/#related-pages","text":"Local Inference Sliding Window Parameters Task and Solution Manage Model Zoo","title":"Related Pages"},{"location":"user-guide/installation/","text":"Installation This guide walks you through installing Napari-OmniEM from a downloaded archive and preparing the runtime environment. Step 1: Download Napari-OmniEM Download the napari-omniem.zip archive from the given google driver folder xz and extract it to a local directory: unzip napari-omniem.zip cd napari-omniem Step 2: Create a Conda Environment We recommend using conda to manage dependencies. conda create -n napari-omniem python=3.11 -y conda activate napari-omniem Step 3: Install PyTorch with CUDA Support Install PyTorch with the appropriate CUDA version for your system. # Example: CUDA 11.8 pip install torch torchvision torchaudio \\ --index-url https://download.pytorch.org/whl/cu118 Visit PyTorch to find the correct installation command for your GPU and CUDA version. If you do not have a GPU, install the CPU-only version instead. Step 4: Install Napari Install napari and its GUI dependencies pip install napari[all] Step 5: Install Napari-OmniEM Install the plugin in editable (development) mode: pip install -e . This makes the plugin immediately available in napari. Step 6: Launch Napari Start napari from the command line: napari Once napari is running, you should be able to find OmniEM in the plugin menu. Notes Ensure that your GPU drivers and CUDA runtime are properly installed before installing PyTorch with CUDA support. For large-volume inference, sufficient GPU memory and disk space are required. Multi-GPU inference is supported when multiple CUDA devices are available.","title":"Installation"},{"location":"user-guide/installation/#installation","text":"This guide walks you through installing Napari-OmniEM from a downloaded archive and preparing the runtime environment.","title":"Installation"},{"location":"user-guide/installation/#step-1-download-napari-omniem","text":"Download the napari-omniem.zip archive from the given google driver folder xz and extract it to a local directory: unzip napari-omniem.zip cd napari-omniem","title":"Step 1: Download Napari-OmniEM"},{"location":"user-guide/installation/#step-2-create-a-conda-environment","text":"We recommend using conda to manage dependencies. conda create -n napari-omniem python=3.11 -y conda activate napari-omniem","title":"Step 2: Create a Conda Environment"},{"location":"user-guide/installation/#step-3-install-pytorch-with-cuda-support","text":"Install PyTorch with the appropriate CUDA version for your system. # Example: CUDA 11.8 pip install torch torchvision torchaudio \\ --index-url https://download.pytorch.org/whl/cu118 Visit PyTorch to find the correct installation command for your GPU and CUDA version. If you do not have a GPU, install the CPU-only version instead.","title":"Step 3: Install PyTorch with CUDA Support"},{"location":"user-guide/installation/#step-4-install-napari","text":"Install napari and its GUI dependencies pip install napari[all]","title":"Step 4: Install Napari"},{"location":"user-guide/installation/#step-5-install-napari-omniem","text":"Install the plugin in editable (development) mode: pip install -e . This makes the plugin immediately available in napari.","title":"Step 5: Install Napari-OmniEM"},{"location":"user-guide/installation/#step-6-launch-napari","text":"Start napari from the command line: napari Once napari is running, you should be able to find OmniEM in the plugin menu.","title":"Step 6: Launch Napari"},{"location":"user-guide/installation/#_1","text":"","title":""},{"location":"user-guide/installation/#notes","text":"Ensure that your GPU drivers and CUDA runtime are properly installed before installing PyTorch with CUDA support. For large-volume inference, sufficient GPU memory and disk space are required. Multi-GPU inference is supported when multiple CUDA devices are available.","title":"Notes"},{"location":"user-guide/local/","text":"","title":"Local Inference"},{"location":"user-guide/manage/","text":"","title":"Task/solution Management"},{"location":"user-guide/sw/","text":"","title":"Parameters for Sliding Window"}]}